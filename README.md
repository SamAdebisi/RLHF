# ðŸ§  RLHF Implementation Projects

A curated collection of foundational and advanced Reinforcement Learning (RL) and Reinforcement Learning with Human Feedback (RLHF) implementations. This repository covers:

- ðŸŸ© Direct Preference Optimization (DPO)
- ðŸŸ¦ Deep Q-Networks (DQN)
- ðŸŸ§ Monte Carlo vs Temporal Difference (MC vs TD)
- ðŸŸ¥ Proximal Policy Optimization (PPO)
- ðŸŸ¨ RLHF Pipeline
- ðŸŸª Value Iteration (FrozenLake)

---

## ðŸ“ Project Structure

â”œâ”€â”€ DPO_Code/                       # Direct Preference Optimization
â”œâ”€â”€ DQN_Code/                       # Deep Q-Network
â”œâ”€â”€ MCvsTD/                  # Monte Carlo vs Temporal Difference
â”œâ”€â”€ PPO_Code/                       # Proximal Policy Optimization
â”œâ”€â”€ RLHF_Code/                      # Reinforcement Learning with Human Feedback
â”œâ”€â”€ value_iteration_FrozenLake/ # Value Iteration on FrozenLake
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

---

## ðŸ“Œ Module Overviews

### ðŸ”¹ DPO (Direct Preference Optimization)
- Implements Direct Preference Optimization, a scalable algorithm for aligning models using human preferences.
- ðŸš€ Key Features:
  - Preference datasets
  - DPO loss vs traditional RLHF approaches
  - Evaluation metrics for preference alignment

### ðŸ”¹ DQN (Deep Q-Network)
- Classic deep reinforcement learning algorithm implemented using PyTorch.
- ðŸ§  Core Concepts:
  - Experience Replay
  - Target Network updates
  - Îµ-greedy exploration
- ðŸ§ª Tested Environments: `CartPole-v1`, `LunarLander-v2`

### ðŸ”¹ MC vs TD (Monte Carlo vs Temporal Difference)
- A pedagogical comparison of Monte Carlo and TD(0) learning techniques.
- ðŸ“Š Focus:
  - FrozenLake-v1 environment
  - Value estimation
  - Convergence speed and stability

### ðŸ”¹ PPO (Proximal Policy Optimization)
- Policy gradient method that uses a clipped objective for more stable updates.
- ðŸ” Details:
  - Generalized Advantage Estimation (GAE)
  - Continuous action environments
  - OpenAI Gym integration

### ðŸ”¹ RLHF (Reinforcement Learning with Human Feedback)
- End-to-end RLHF implementation:
  - Pretraining (optional)
  - Synthetic or manual preference generation
  - Reward model training
  - Fine-tuning using PPO or DPO
- ðŸ¤ Human-in-the-loop Simulation Supported

### ðŸ”¹ FrozenLake_ValueIteration
- Tabular method using Value Iteration algorithm on FrozenLake.
- ðŸ§® Includes:
  - Optimal policy visualization
  - Value convergence tracking
  - Heatmaps and trajectories

---

## ðŸ“¦ Installation

### ðŸ›  Requirements

```bash
python>=3.8
torch>=1.10
numpy
matplotlib
gym==0.21
scikit-learn
transformers

